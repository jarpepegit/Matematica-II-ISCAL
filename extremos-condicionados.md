---
description: >-
  Otimiza√ß√£o condicionada. Fun√ß√£o objetivo. Restri√ß√£o de igualdade. 
  Multiplicador de Lagrange. Fun√ß√£o de Lagrange (Lagrangeano). Matriz Hessiana
  Orlada.
---

# üóÉÔ∏è Extremos condicionados

## Qual √© o problema que vamos estudar?

Queremos maximizar ou minimizar (**otimizar**) uma fun√ß√£o real em v√°rias vari√°veis sujeita a (s.a.) **uma** **restri√ß√£o** de igualdade. Para fixar ideias, a formula√ß√£o matem√°tica do **problema de otimiza√ß√£o condicionada** para fun√ß√µes em **duas vari√°veis** √© a seguinte

$$
\begin{matrix}\displaystyle\max_{(x,y)\in\operatorname{Dom}f}\,f(x,y) \\[2ex]\text{s.a.}\quad g(x,y)=b\end{matrix}\qquad\text{ou}\qquad\begin{matrix}\displaystyle\min_{(x,y)\in\operatorname{Dom}f}\,f(x,y) \\[2ex]\text{s.a.}\quad g(x,y)=b\end{matrix}
$$

{% hint style="info" %}
&#x20;**Nomes.**

* $$f(x,y)$$: fun√ß√£o **objetivo**.
* $$g(x,y)$$: fun√ß√£o **restri√ß√£o.**
* $$D=\big\{(x,y)\in\operatorname{Dom}f \,:\, g(x,y)=b\big\}$$: **regi√£o de factibilidade**.
* A **solu√ß√£o** do problema de otimiza√ß√£o condicionada, isto √©, por exemplo,$$(x^\ast,y^\ast)=\begin{matrix}\displaystyle\max_{(x,y)\in\operatorname{Dom}f}f(x,y) \\[2ex]\text{s.a.}\quad g(x,y)=b\end{matrix}$$ chama-se de **ponto m√°ximo condicionado** de $$f$$.
* $$f(x^\ast,y^\ast)$$: **valor m√°ximo condicionado** de $$f$$.
{% endhint %}

## Como resolvemos o problema?

<details>

<summary><span data-gb-custom-inline data-tag="emoji" data-code="2139">‚ÑπÔ∏è</span> Marco te√≥rico importante.</summary>

Se $$g:\mathbb{R}^2\to\mathbb{R}$$ √© uma fun√ß√£o **cont√≠nua** no seu dom√≠nio, ent√£o o conjunto $$D$$ dos pontos $$(x,y)\in\operatorname{Dom}f$$ que satisfazem a **equa√ß√£o** $$g(x,y)=b$$ √© um conjunto **fechado** em $$\mathbb{R}^2$$. Geometricamente, o conjunto $$D$$ √© um **caminho** em $$\mathbb{R}^2$$ (uma especie de reta ou segmento de reta "curvado"), como por exemplo, uma par√°bola, uma elipse, um segmento de reta, et&#x63;**.** Neste sentido, a  regi√£o de factibilidade $$D$$ √© un lugar geom√©trico de **dimens√£o** 1. Se, al√©m disso, $$D$$ for **limitado**, ent√£o diremos que $$D$$ √© um conjunto **compacto** em $$\mathbb{R}^2$$.  O teorema an√°logo ao [Teorema de Weierstrass](extremos-relativos-e-absolutos.md#teorema-analogo-ao-do-weierstrass-para-funcoes-numa-variavel) para fun√ß√µes em v√°rias vari√°veis afirma que se $$f$$ √© uma fun√ß√£o cont√≠nua em $$D$$, ent√£o $$f$$ atinge os seus valores m√°ximo e m√≠nimo em $$D$$. Neste t√≥pico iremos muitas vezes assumir, devido a poss√≠veis interpreta√ß√µes em situa√ß√µes concretas (econ√≥micas por exemplo), que $$x\ge 0$$ e $$y\ge 0$$. Estas duas restri√ß√µes de desigualdade junto com a restri√ß√£o de igualdade fazem com que $$D$$ seja compacto, e, portanto, se conseguirmos encontrar pontos candidatos a solu√ß√£o (**pontos cr√≠ticos**) de um problema de otimiza√ß√£o condicionado, **bastar√°** com calcular os valores que a fun√ß√£o objetivo toma nesses pontos  para logo escolher o **maior** deles como **valor m√°ximo** e o **menor** como **valor m√≠nimo**.

</details>

**Solu√ß√£o gr√°fica.**

No **caso de duas vari√°veis** podemos encontrar a solu√ß√£o gr√°ficamente.

* Desenhe algumas curvas de n√≠vel da fun√ß√£o objetivo $$f(x,y)$$ que intersectem a restri√ß√£o $$g(x,y)=c$$ para ter uma ideia de como varia o valor de $$f$$.
* Desenhe a restri√ß√£o $$g(x,y)=c$$ no plano XY.&#x20;
* Das curvas de n√≠vel de $$f$$ que intersectam a restri√ß√£o $$g(x,y)=c$$, escolha aquela de maior valor (problema de maximiza√ß√£o) ou a de menor valor (problema de minimiza√ß√£o).

> :thinking: <mark style="color:purple;">Suponhamos que somos um desastre desenhando curvas, ou que elas n√£o s√£o assim t√£o simples de desenhar.  H√° algum m√©todo sistem√°tico  para encontrar a solu√ß√£o √≥tima nos problemas de otimiza√ß√£o condicionada?</mark>
>
> Sim.

Antes de estudar tal forma, vejamos primeiro um par de situa√ß√µes.

### Restri√ß√£o de igualdade linear, Fun√ß√£o objetivo linear

<details>

<summary>Exemplo 1.</summary>

Uma companhia produce dois tipos de produtos: A e B. O lucro por unidade do produto A que a companhia deseja obter √© de 10 euros, e por unidade do produto B, de 15 euros. A companhia tem um or√ßamento de 2000 euros para custos de produ√ß√£o. Sabendo que o custo de produzir uma unidade do produto A √© de 5 euros, e para produzir uma unidade do produto B o custo √© de 7 euros, quantas unidades de cada produto deve produzir a companhia de modo que maximize o lucro usando tudo o or√ßamento dispon√≠vel?

***

**Formula√ß√£o matem√°tica:**&#x20;

* $$x$$: unidades produzidas de A,&#x20;
* $$y$$: unidades produzidas de B,&#x20;
* $$f(x,y)=10x+15y$$: objetivo (lucro),
* $$g(x,y)=5x+7y=2000$$: restri√ß√£o (or√ßamental).

Queremos maximizar $$\left\{\begin{matrix}f(x,y)=10x+15y\\[2ex] \text{s.a.}\,\,5x+7y=2000\end{matrix}\right.$$.&#x20;

***

**Solu√ß√£o gr√°fica**:

Note-se que as curvas de n√≠vel de $$f$$ s√£o retas. O ponto que maximiza o valor de $$f$$ ocorre na  interse√ß√£o da restri√ß√£o $$g(x,y)=c$$ com o eixo coordenado Y (como se pode observar no [aplicativo](extremos-condicionados.md#restricao-de-igualdade-linear-funcao-objetivo-linear) abaixo). Isto √© assim porque, verifica-se que o valor da fun√ß√£o objetivo cresce assim que subimos pelo gr√°fico da restri√ß√£o, e, dado que n√£o podemos continuar indefinidamente a subir, temos de parar na interse√ß√£o com o eixo coordenado Y, pois intr√≠nsecamente assumimos que tanto $$x$$ como $$y$$ devem ser n√£o negativos (dado que n√£o faz sentido produzir unidades negativas dos produtos A e B).&#x20;

**Solu√ß√£o anal√≠tica**:

A partir da restri√ß√£o escrevemos $$y=-\frac{5}{7}x+\frac{2000}{7}$$ e substituimos na fun√ß√£o objetivo para definir a fun√ß√£o&#x20;

$$\tilde{f}(x)=f\big(x,-\frac{5}{7}x+\frac{2000}{7}\big)=10x+15\big(-\frac{5}{7}x+\frac{2000}{7}\big)=-\frac{5}{7}x+\frac{2000\times15}{7}$$.

O nosso problema √© agora maximizar uma fun√ß√£o unidimensional, e, como se trata de uma fun√ß√£o "**linear**" com declive negativo, claramente, o m√°ximo de $$\tilde{f}$$ √© atingido na interse√ß√£o com o eixo Y, ou seja, na fronteira, quando $$x^\ast=0$$, e portanto $$y^\ast=\frac{2000\times15}{7}=f(x^\ast,y^\ast).$$



</details>

{% embed url="https://www.geogebra.org/classic/ngb4fhcy" %}
&#x20;Regi√£o de factibilidade: segmento de reta a vermelho no primeiro quadrante. Fun√ß√£o objetivo linear: os extremos ocorrem na **fronteira** da regi√£o de factibilidade.
{% endembed %}

> :information\_source: O problema de otimizar uma fun√ß√£o objetivo linear sujeita a restri√ß√µes lineares (de igualdade e desigualdade) √© o teor da Programa√ß√£o Linear.

### Fun√ß√£o objetivo n√£o-linear

<details>

<summary>Exemplo 2.</summary>

O lucro pela produ√ß√£o de um determinado bem numa determinada empresa segue o modelo de Cobb-Douglas[^1]

&#x20;                                 $$\textcolor{blue}{f(x,y)=b\,x^{\alpha}y^{1-\alpha}}$$,

onde $$\bm{x}$$ representa horas de trabalho, $$\bm{y}$$ o capital investido, e $$\bm{b}$$ o fator de produtividade (ou pre√ßo da produ√ß√£o, que se assume diferente de zero). O par√¢metro $$0<\alpha<1$$, representa a elasticidade da produ√ß√£o em rela√ß√£o ao trabalho e ao capital. A empresa deseja **maximizar** **o lucro** sujeito √†  **restri√ß√£o or√ßamental**&#x20;

&#x20;                                 $$\textcolor{red}{\mathsf{orc}=g(x,y)=T x + K y}$$,&#x20;

onde $$T$$ representa o custo do trabalho, e $$K$$, o custo do capital investidos.

***

Olhando para as curvas de n√≠vel de $$f$$ (experimente com o aplicativo abaixo), verifica-se que a fun√ß√£o objetivo √© **maximizada** quando a curva de n√≠vel √© **tangente** √† curva de restri√ß√£o or√ßamental.

:thinking: <mark style="color:purple;">Digamos que o ponto de tang√™ncia √©</mark> $$\textcolor{blue}{p=(x_c,y_c)}$$<mark style="color:purple;">. Como o encontramos?</mark>&#x20;

A observa√ß√£o chave √© que **no ponto de tang√™ncia**, o gradiente de $$f$$ √© **paralelo** ao gradiente de $$g$$ em forma **n√£o trivial**, isto √©

&#x20;                       $$\colorbox{yellow}{$\nabla f(p)=\lambda\nabla g(p)$}$$, para algum $$\lambda\in\mathbb{R}\setminus\{0\}$$.

Ou seja, &#x20;

$$\begin{matrix}f_x(x_c,y_c) &=& \lambda\, g_x(x_c,y_c)\\[2ex] f_y(x_c,y_c) &=& \lambda\, g_y(x_c,y_c)\end{matrix}\qquad\Longleftrightarrow\qquad\begin{matrix}\alpha b\, x_c^{\alpha-1}y_c^{1-\alpha} &=& \lambda\, T\\[2ex] (1-\alpha) b \,x_c^{\alpha}y_c^{-\alpha} &=& \lambda\,K\end{matrix}$$

Como $$\lambda\neq 0$$, temos que $$x_c\neq 0$$ e $$y_c\neq 0$$, e ent√£o podemos escrever

&#x20;                              $$\dfrac{\alpha \cancel{b}\, x_c^{\alpha-1}y_c^{1-\alpha}}{(1-\alpha)\cancel{b}\, x_c^\alpha y_c^{-\alpha}}=\dfrac{T}{K}\quad\Longrightarrow\quad y_c=\dfrac{(1-\alpha)T}{\alpha K}x_c$$.

Como $$(x_c,y_c)\in D$$ (a regi√£o de factibilidade), por defini√ß√£o deve satisfazer a restri√ß√£o or√ßamental, e ent√£o

&#x20;                    $$\mathsf{orc}=Tx_c+Ky_c=Tx_c+K\frac{(1-\alpha)T}{\alpha K}x_c\quad\Longrightarrow\quad x_c=\dfrac{\alpha\,\mathsf{orc}}{T}$$.

Logo, o ponto de tang√™ncia √©&#x20;

&#x20;                                           $$\colorbox{yellow}{$(x_c,y_c)=\Big(\frac{\alpha\,\mathsf{orc}}{T},\frac{(1-\alpha)\,\mathsf{orc}}{K}\Big)$}.$$

:thinking: <mark style="color:purple;">Tudo isto √© muito bonito, mas como sabemos se efetivamente √© a solu√ß√£o do problema de otimiza√ß√£o condicionada? Por outras palavras, como sabemos se √© um ponto m√°ximo da fun√ß√£o objetivo? Podia ser um ponto m√≠nimo, ou nenhum deles? Qual √© o crit√©rio que nos permite saber se o ponto encontrado √© um m√°ximo ou um m√≠nimo?</mark>

Note-se que encontramos o ponto $$(x_c,y_c)$$ resolvendo uma **condi√ß√£o de tang√™ncia** que graficamente se verifica para um ponto extremo da fun√ß√£o objetivo, ou seja, usando uma **condi√ß√£o necess√°ria:** se sabemos a priori que $$p$$ √© um ponto extremo restringido de $$f$$ (sujeita √† uma restri√ß√£o), ent√£o os gradientes da fun√ß√£o objetivo e da fun√ß√£o restri√ß√£o em $$p$$ s√£o paralelos.&#x20;

:thinking: <mark style="color:purple;">Uhmm! ponto de tang√™ncia...isto cheira a ponto cr√≠tico, como no caso da otimiza√ß√£o irrestrita. Ah! ent√£o?, existe algo parecido neste contexto ao crit√©rio das segundas derivadas da otimiza√ß√£o irrestrita?</mark>

Sim, existe! Veremos isto mais adiante. A ideia √© construir uma nova fun√ß√£o (a fun√ß√£o de Lagrange ou **Lagrangeano**) com mais uma vari√°vel, $$\bm{\lambda}$$ (**multiplicador de Lagrange**), a partir da fun√ß√£o objetivo e da fun√ß√£o restri√ß√£o. Desta maneira, um problema de otimiza√ß√£o restrita em $$\mathbb{R}^2$$ torna-se num problema de otimiza√ß√£o irrestrita em $$\mathbb{R}^3$$. O ponto de tang√™ncia encontrado anteriormente √© parte do **ponto cr√≠tico** $$(x_c.y_c,\lambda_c)$$ do Lagrangeano, &#x65;**,** neste context&#x6F;**,** o **Hessiano do Lagrangeano** no ponto $$(x_c.y_c,\lambda_c)$$ dar√° a informa√ß√£o que nos falta para determinar se o ponto cr√≠tico $$(x_c,y_c)$$ √© um m√°ximo ou um m√≠nimo da fun√ß√£o $$f(x,y)$$ sujeita √† restri√ß√£o $$g(x,y)=\mathsf{orc}$$.

Para terminar, emulando o [Exemplo 1](extremos-condicionados.md#exemplo-1), da restri√ß√£o $$\textcolor{red}{\mathsf{orc}=T x + K y}$$, segue-se que  $$y=-\frac{T}{K}x+\frac{\mathsf{orc}}{K}$$. Substituimos na fun√ß√£o objetivo $$\textcolor{blue}{f(x,y)=b\,x^{\alpha}y^{1-\alpha}}$$ e definimos a fun√ß√£o $$\widetilde{f}:\mathbb{R}\to\mathbb{R}$$ pela express√£o $$\widetilde{f}(x)=b\,x^\alpha\Big(\frac{\mathsf{orc}}{K}-\frac{T}{K}x\Big)^{1-\alpha}$$. Logo, o nosso problema  original de otimiza√ß√£o condicionada √© **equivalente** a um problema de otimiza√ß√£o em $$\mathbb{R}$$. Deixo os detalhes ao leitor.

</details>

{% embed url="https://www.geogebra.org/classic/frwz4tqp" %}
Fun√ß√£o objetivo n√£o linear. Os extremos da fun√ß√£o objetivo ocorrem no **interior** da regi√£o de factibilidade (o segmento de reta a vermelho no primeiro quadrante).
{% endembed %}

## Fun√ß√£o de Lagrange

At√© agora temos estado a trabalhar com fun√ß√µes em $$\mathbb{R}^2$$. Para perceber melhor as defini√ß√µes e resultados seguintes,  consideremos agora fun√ß√µes em mais do que duas vari√°veis. Por exemplo, suponhamos que queremos resolver o seguinte problema

$$
\begin{matrix}\displaystyle\max_{(w,x,y,z)\in\operatorname{Dom}f\subset\mathbb{R}^4}\,f(w,x,y,z) \\[2ex]\text{s.a.}\quad g(w,x,y,z)=b_1 \\ \,\,\qquad h(w,x,y,z)=b_2\end{matrix}
$$

Para fixar ideias, digamos que (o seguinte exemplo √© retirado de [https://www.math.cmu.edu/\~hanifc/NotesOnHessians.pdf](https://www.math.cmu.edu/~hanifc/NotesOnHessians.pdf))

$$
\begin{array}{lcl} f(w,x,y,z) &=& -w^2-x^2-y^2-z^2 \\ \qquad\small\mathsf{e} \\ g_1(w,x,y,z) &=& 4w-3y+z=-15\\
g_2(w,x,y,z) &=& -2x-y+z=-5\end{array}
$$

A **fun√ß√£o de Lagrange** (ou **Lagrangeano**) associada ao nosso problema √©

$$
\mathcal{L}(\lambda_1,\lambda_2;w,x,y,z)=f(w,x,y,z)-\lambda_1\big(g_1(w,x,y,z)-b_1\big)-\lambda_2\big(g_2(w,x,y,z)-b_2\big).
$$

{% hint style="info" %}
:mag\_right: <mark style="color:red;">Note-se</mark> que $$\mathcal{L}:\mathbb{R}^6\to\mathbb{R}$$, enquanto que $$f,g_1,g_2: \mathbb{R}^4\to\mathbb{R}$$. Repare-se ainda que $$\mathcal{L}$$ √© irrestrita (incorporou as restri√ß√µes $$g_1=b_1$$ e $$g_2=b_2$$ na sua constru√ß√£o). A ideia √©  **maximizar** $$\mathcal{L}$$ para que a partir de ai, consigamos de [alguma forma](#user-content-fn-2)[^2] maximizar $$f$$ sujeita √†s restri√ß√µes dadas.
{% endhint %}

Sejam $$\bm{\lambda}=(\lambda_1,\lambda_2)$$ e $$\bm{p}=(w,x,y,z)$$. Recordemos que $$(\bm{\lambda},\bm{p})$$ √© um **ponto cr√≠tico** de $$\mathcal{L}$$  se $$\nabla\mathcal{L}(\bm{\lambda},\bm{p})=\vec{0}$$. Deste modo, para encontrar os pontos cr√≠ticos de $$\mathcal{L}$$, temos de resolver o seguinte sistema de equa√ß√µes

$$
\small\colorbox{yellow}{$\nabla\mathcal{L}(\lambda,p)=\vec{0}$}\,\Longleftrightarrow\,\begin{bmatrix}\mathcal{L}_{\lambda_1}(\lambda,p) \\ \mathcal{L}_{\lambda_2}(\lambda,p) \\ \mathcal{L}_w(\lambda,p) \\ \mathcal{L}_x(\lambda,p) \\ \mathcal{L}_y(\lambda,p) \\ \mathcal{L}_z(\lambda,p)\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\\0\\0\end{bmatrix}\,\Longleftrightarrow\,\begin{bmatrix}b_1-g_1(p)\\b_2-g_2(p)\\f_w(p)-\lambda_1{g_1}_w(p)-\lambda_2{g_2}_w(p) \\ f_x(p)-\lambda_1{g_1}_x(p)-\lambda_2{g_2}_x(p) \\ f_y(p)-\lambda_1{g_1}_y(p)-\lambda_2{g_2}_y(p) \\ f_z(p)-\lambda_1{g_1}_z(p)-\lambda_2{g_2}_z(p)\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\\0\\0\end{bmatrix} .
$$

As duas primeiras equa√ß√µes s√£o as restri√ß√µes de igualdade dadas,&#x20;

$$
\colorbox{yellow}{$g_1(p)=b_1$}\qquad\mathsf{e}\qquad \colorbox{yellow}{$g_2(p)=b_2$} .
$$

As restantes equa√ß√µes s√£o equivalentes a dizer que o vetor $$\nabla f(p)$$ pode ser exprimido como uma **combina√ß√£o linear** dos vetores $$\nabla g_1(p)$$ e $$\nabla g_2(p)$$,

$$
\colorbox{yellow}{$\nabla f(p) = \lambda_1\nabla g_1(p) + \lambda_2\nabla g_2(p) $}.
$$

Se os vetores  $$\nabla g_1(p)$$ e $$\nabla g_2(p)$$ s√£o **linearmente independentes**, ent√£o tal combina√ß√£o linear √© **√∫nica.** Esta √© a **generaliza√ß√£o da condi√ß√£o de tang√™ncia** que vimos no [Exemplo 2](extremos-condicionados.md#exemplo-2). Portanto, se $$(\bm{\lambda},\bm{p})$$ √© um ponto cr√≠tico do Lagrangeano $$\mathcal{L}$$, ent√£o $$\bm{p}$$ √© um **ponto cr√≠tico condicionado** ("ponto de tang√™ncia") de $$f$$, e viceversa.&#x20;

{% hint style="warning" %}
Nos problemas que nos interessam, usualmente esta condi√ß√£o de independ√™ncia linear √© satisfeita.
{% endhint %}

No caso particular das fun√ß√µes $$f, g_1, g_2$$ dadas, obtemos o seguinte sistema de equa√ß√µes

$$
\begin{array}{lcl}
\mathcal{L}_{\lambda_1}=-15 - 4w + 3y - z &=& 0 \\
\mathcal{L}_{\lambda_2}=-5 + 2x +y - z &=& 0 \\
\mathcal{L}_{w}=-2w - 4\lambda_1 &=& 0 \\
\mathcal{L}_{x}=-2x + 2\lambda_2 &=& 0 \\
\mathcal{L}_{y}=-2y + 3\lambda_1 + \lambda_2 &=& 0\\
\mathcal{L}_{z}=-2z - \lambda_1 - \lambda_2 &=& 0
\end{array}
$$

{% hint style="info" %}
:mag\_right:  <mark style="color:red;">Note-se</mark> que neste caso particular temos um sistema de equa√ß√µes lineares com $$6$$ inc√≥gnitas. Refrescando os nossos conhecimentos de Matem√°tica I, podemos usar, por exemplo, o algoritmo de Gauss para o resolver. Verifique que a solu√ß√£o √©  $$(\bm{\lambda,p})=(\lambda_1,\lambda_2,w,x,y,z)=\colorbox{yellow}{$(-1,-1,2,-1,-2,1)$}$$.
{% endhint %}

## Matriz Hessiana orlada

Calculemos agora a **matriz Hessiana do Lagrangeano**. Ela √© dada por

$$
\mathcal{H}_{\mathcal{L}}=\begin{bmatrix}\mathcal{L}_{\lambda_1\lambda_1} & \mathcal{L}_{\lambda_1\lambda_2} & \mathcal{L}_{\lambda_1w} & \mathcal{L}_{\lambda_1x} & \mathcal{L}_{\lambda_1y} & \mathcal{L}_{\lambda_1z} \\ \mathcal{L}_{\lambda_2\lambda_1} & \mathcal{L}_{\lambda_2\lambda_2} & \mathcal{L}_{\lambda_2w} & \mathcal{L}_{\lambda_2x} & \mathcal{L}_{\lambda_2y} & \mathcal{L}_{\lambda_2z} \\ \mathcal{L}_{w\lambda_1} & \mathcal{L}_{w\lambda_2} & \mathcal{L}_{ww} & \mathcal{L}_{wx} & \mathcal{L}_{wy} & \mathcal{L}_{wz} \\ \mathcal{L}_{x\lambda_1} & \mathcal{L}_{x\lambda_2} & \mathcal{L}_{xw} & \mathcal{L}_{xx} & \mathcal{L}_{xy} & \mathcal{L}_{xz} \\ \mathcal{L}_{y\lambda_1} & \mathcal{L}_{y\lambda_2} & \mathcal{L}_{yw} & \mathcal{L}_{yx} & \mathcal{L}_{yy} & \mathcal{L}_{yz} \\ \mathcal{L}_{z\lambda_1} & \mathcal{L}_{z\lambda_2} & \mathcal{L}_{zw} & \mathcal{L}_{zx} & \mathcal{L}_{zy} & \mathcal{L}_{zz}\end{bmatrix}
$$

$$
\mathcal{H}_{\mathcal{L}}=\begin{bmatrix}\colorbox{yellow}{0} & \colorbox{yellow}{0} & -{g_1}_w & -{g_1}_x & -{g_1}_y & -{g_1}_z \\ \colorbox{yellow}{0} & \colorbox{yellow}{0} & -{g_2}_w & -{g_2}_x & -{g_2}_y & -{g_2}_z \\ -{g_1}_w & -{g_2}_w & \colorbox{lightgreen}{$\mathcal{L}_{ww}$} & \colorbox{lightgreen}{$\mathcal{L}_{wx}$} & \colorbox{lightgreen}{$\mathcal{L}_{wy}$} & \colorbox{lightgreen}{$\mathcal{L}_{wz}$} \\ -{g_1}_x & -{g_2}_x & \colorbox{lightgreen}{$\mathcal{L}_{xw}$} & \colorbox{lightgreen}{$\mathcal{L}_{xx}$} & \colorbox{lightgreen}{$\mathcal{L}_{xy}$} & \colorbox{lightgreen}{$\mathcal{L}_{xz}$} \\ -{g_1}_y & -{g_2}_y & \colorbox{lightgreen}{$\mathcal{L}_{yw}$} & \colorbox{lightgreen}{$\mathcal{L}_{yx}$} & \colorbox{lightgreen}{$\mathcal{L}_{yy}$} & \colorbox{lightgreen}{$\mathcal{L}_{yz}$} \\ -{g_1}_z & -{g_2}_z & \colorbox{lightgreen}{$\mathcal{L}_{zw}$} & \colorbox{lightgreen}{$\mathcal{L}_{zx}$} & \colorbox{lightgreen}{$\mathcal{L}_{zy}$} & \colorbox{lightgreen}{$\mathcal{L}_{zz}$}\end{bmatrix}
$$

{% hint style="info" %}
:mag\_right: <mark style="color:red;">Note-se</mark> a estrutura do Hessiano do Lagrangeano mostrado acima. √â uma matriz de tamanho $$\bm{n+m=6}$$, com $$\bm{n}$$ igual ao n√∫mero de vari√°veis de $$f$$  (neste caso $$\bm{n=4}$$), e $$\bm{m}$$ igual ao n√∫mero de restri√ß√µes (neste caso $$\bm{m=2}$$). No canto superior esquerdo de $$\mathcal{H}_{\mathcal{L}}$$, temos um **bloco de zeros** de tamanho $$\bm{m}$$, e, a seguir o bloco, as $$\bm{m}$$ **primeiras** **linhas e colunas** de $$\mathcal{H}_{\mathcal{L}}$$ cont√™m as **derivadas das restri√ß√µes.** Al√©m disso, observamos tamb√©m um **bloco** de tamanho $$\bm{n}$$ com as **segundas derivadas de** $$\bm{\mathcal{L}}$$ em rela√ß√£o √†s vari√°veis de $$f$$. Agora, fazendo $$\bm{g}=(g_1,\cdots,g_m)$$, denotando por $$\bm{0}$$ a matriz de zeros de tamanho $$\bm{m}$$, escrevendo $$\bm{\nabla g = \begin{bmatrix}\nabla g_1 \\ \vdots\\ \nabla g_m\end{bmatrix}}$$, e finalmente, denotando por $$\bm{x}$$ as vari√°veis de $$f$$ (no exemplo dado, $$\bm{x}=(w,x,y,z)$$), podemos escrever o Hessiano $$\mathcal{H}_{\mathcal{L}}$$ de maneira mais compacta da forma seguinte

$$\mathcal{H}_{\mathcal{L}}=\begin{bmatrix}\bm{0} & \bm{-\nabla g} \\[2ex] (\bm{-\nabla g})^T & \bm{\dfrac{\partial^2\mathcal{L}}{\partial x^2}}\end{bmatrix}$$. O **Hessiano** $$\bm{\mathcal{H}_{\mathcal{L}}}$$ recebe o nome de **Hessiano orlado**, devido ao que acontece nas **margens** superior e esquerda da matriz.
{% endhint %}

No caso particular do exemplo, temos

$$
\mathcal{H}_{\mathcal{L}}=\begin{bmatrix}\colorbox{yellow}{0} & \colorbox{yellow}{0} & -4 & 0 & 3 & -1 \\ \colorbox{yellow}{0} & \colorbox{yellow}{0} & 0 & 2 & 1 & -1 \\ -4 & 0 & \colorbox{lightgreen}{-2} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} \\ 0 & 2 & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{-2} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} \\ 3 & 1 & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{-2} & \colorbox{lightgreen}{0} \\ -1 & -1 & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{0} & \colorbox{lightgreen}{-2}\end{bmatrix}
$$

Repare-se que neste caso particular, o Hessiano $$\mathcal{H}_{\mathcal{L}}$$ √© uma matriz constante, n√£o depende das coordenadas do ponto cr√≠tico $$\bm{(\lambda,p)}$$. Contudo, n√£o se deve esquecer que <mark style="color:red;">**primeiro calculamos a matriz Hessiana de forma geral, e depois a avaliamos no ponto cr√≠tico encontrado.**</mark> Procedemos desta forma  porque a ideia √© determinar a **natureza do ponto cr√≠tico** analizando a natureza da matriz resultante. No geral, o Hessiano $$\mathcal{H}_{\mathcal{L}}{\small(\lambda,p)}$$ ir√° depender das coordenadas do ponto cr√≠tico $$(\lambda,p)$$.

{% hint style="info" %}
Recordemos que os pontos extremos de uma fun√ß√£o sem restri√ß√µes, s√£o caracterizados (entre os pontos cr√≠ticos com matriz Hessiana n√£o singular) pelos **sinais** dos **menores principais l√≠deres**, os quais determinam se a matriz Hessiana √© **definida positiva** (e ent√£o o ponto cr√≠tico √© um **m√≠nimo**) ou definida negativa (e ent√£o o ponto cr√≠tico √© um **m√°ximo**). No caso do Hessiano orlado, n√£o h√° forma que seja quer definida positiva quer definida negativa. Isto se deve ao bloco de zeros que necessariamente aparece nesta matriz. Por√©m, existe um [resultado](extremos-condicionados.md#condicao-suficiente-para-pontos-extremos) que contorna este problema.
{% endhint %}

## Condi√ß√£o suficiente para extremos condicionados

Denotemos por $$B_r(\lambda,p)$$ o $$(m+r)$$-√©simo menor principal l√≠der de $$\mathcal{H}_{\mathcal{L}}(\lambda,p)$$. Assumamos que $$\bm{(\lambda,p)}$$ √© um ponto cr√≠tico de $$\mathcal{L}:\mathbb{R}^m\times\mathbb{R}^n\to\mathbb{R}$$. Suponhamos que tanto a fun√ß√£o objetivo $$f:\mathbb{R}^n\to\mathbb{R}$$ como as fun√ß√µes de restri√ß√£o $$g_1,\ldots,g_m:\mathbb{R}^n\to\mathbb{R}$$ s√£o de classe $$C^2$$ no dom√≠nio comum a todas elas. E por √∫ltimo, suponhamos que os vetores $$\{\nabla g_1(\bm{p}),\ldots,\nabla g_m(\bm{p})\}$$s√£o linearmente independentes. Ent√£o

* Se $$(-1)^rB_r(\bm{\lambda,p})>0,\forall\,r=m+1,\ldots, n\implies \bm{p}$$ √© um **m√°ximo local** condicionado de $$f$$.
* Se $$(-1)^mB_r(\bm{\lambda,p})>0,\,\forall\,r=m+1,\ldots, n\implies \bm{p}$$ √© um **m√≠nimo local** condicionado de $$f$$.

{% hint style="warning" %}
Portanto, para determinar se um ponto cr√≠tico $$\bm{p}$$ √© um extremo condicionado local de $$f$$,  temos de olhar para os sinais dos **√∫ltimos** $$\bm{n-m}$$ **menores principais l√≠deres do Hessiano orlado** $$\mathcal{H}_{\mathcal{L}}(\bm{\lambda,p})$$.&#x20;
{% endhint %}

No problema de otimiza√ß√£o original temos **4 vari√°veis**, e **2 restri√ß√µes**:  $$\boxed{n=4, m=2, r=3,4}$$. Logo,&#x20;

$$
\scriptsize B_3(\lambda,p)=\begin{vmatrix*}[r] 0 & \phantom{-}0 & -4 & 0 & 3 \\ 0 & 0 & 0 & 2 & 1 \\ -4 & 0 & -2 & 0 & 0 \\ 0 & 2 & 0 & -2 & 0 \\ 3 & 1 & 0 & 0 & -2\end{vmatrix*}_{\tiny 5\times 5}=-232\quad,\quad B_4(\lambda,p)=\begin{vmatrix*}[r]{0} & {0} & -4 & 0 & 3 & -1 \\ {0} & {0} & 0 & 2 & 1 & -1 \\ -4 & 0 & {-2} & {0} & {0} & {0} \\ 0 & 2 & {0} & {-2} & {0} & {0} \\ 3 & 1 & {0} & {0} & {-2} & {0} \\ -1 & -1 & {0} & {0} & {0} & {-2}\end{vmatrix*}_{\tiny 6\times 6}=560
$$

* $$(-1)^rB_r:$$   $$(-1)^3B_3$$ $$\colorbox{lightgreen}{$>0$}$$,  $$(-1)^4B_4$$ $$\colorbox{lightgreen}{$>0$}$$  condi√ß√£o satisfeita. :thumbsup:
* $$(-1)^mB_r:$$   $$(-1)^2B_3$$ $$\colorbox{#F1AFA0}{$<0$}$$,   $$(-1)^2B_4$$ $$\colorbox{lightgreen}{$>0$}$$  condi√ß√£o **n√£o** satisfeita. :thumbsdown:

$$\boxed{\textsf{\textbf{Conclus√£o:}}}$$ O ponto $$\bm{p}=(2,-1,-2,1)$$ √© um **ponto m√°ximo** condicionado local de $$f$$.

## V√≠deo elucidativo

{% embed url="https://www.youtube.com/watch?ab_channel=KhanAcademy&v=yuqB-d5MjZA" %}
V√≠deo criado por Grant Sanderson
{% endembed %}



[^1]: ![](.gitbook/assets/Cobb-Douglas.png)

[^2]: calma! j√° veremos de que forma
